# ChefFusion: Multimodal Foundation Model Integrating Recipe and Food Image Generation

## Abstract
Significant work has been conducted in the domain of food computing, yet these studies typically focus on single tasks such as t2t (instruction generation from food titles and ingredients), i2t (recipe generation from food images), or t2i (food image generation from recipes). None of these approaches integrate all modalities simultaneously.
To address this gap, we introduce a novel food computing foundation model that achieves true multimodality, encompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large language models (LLMs) and pre-trained image encoder and decoder models, our model can perform a diverse array of food computing-related tasks, including food understanding, food recognition, recipe generation, and food image generation.
Compared to previous models, our foundation model demonstrates a significantly broader range of capabilities and exhibits superior performance, particularly in food image generation and recipe generation tasks.


## Case Study and the Architecture of ChefFusion
![case](https://github.com/user-attachments/assets/64556d24-45c2-44be-88de-27dcfa1ed53c "Case Study: ChefFusion demonstrates a wide suite of multimodal capabilities, including food understanding, food recognition, recipe generation, food image generation and multimodal dialogue (left). Example of food images generated by ChefFusion (right).")
**Case Study**: ChefFusion demonstrates a wide suite of multimodal capabilities, including food understanding, food recognition, recipe generation, food image generation and multimodal dialogue (left). Example of food images generated by ChefFusion (right).

![pipeline-1](https://github.com/user-attachments/assets/5910581a-f9c6-443b-b75c-663cca880a64 "The architecture of ChefFusion: (1) Left: training the model to generate recipe by minimizing $l_{r}(x, y)$; (2) Right: training the model to generate food images by minimizing $l_{g}(y)$ and determine whether to produce text or images at each step by minimizing $l_{p}(y)$.")
**The architecture of ChefFusion**: (1) Left: training the model to generate recipe by minimizing $l_{r}(x, y)$; (2) Right: training the model to generate food images by minimizing $l_{g}(y)$ and determine whether to produce text or images at each step by minimizing $l_{p}(y)$.


<p align="center">
<img alt="Inference procedure for ChefFusion: The model takes in image and text inputs, and generate text interleaved with food image." src="https://github.com/user-attachments/assets/0a6c0436-ccc6-4270-8449-50aa5bf7a40d " width=500/>
</p>

**Inference procedure for ChefFusion**: The model takes in image and text inputs, and generate text interleaved with food image.

## How to Set Up
### Environment
Set up a new virtualenv, and install required libraries:
```
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Add the `chefFusion` library to PYTHONPATH:
```
export PYTHONPATH=$PYTHONPATH:/home/path/to/chefFusion/
```
## Checkpoints
The checkpoint and model config in `runs/`reproduce the main results reported in our paper.

## Training

### Preparing the Dataset

Our model is trained on the [Recipe1M](https://github.com/torralba-lab/im2recipe?tab=readme-ov-file#recipe1m-dataset) dataset. 

After following the instructions on the website to download the dataset, you will see two files named `layer1.json` and `layer2.json`. If you fail to download the dataset from the link, please contact the author of the paper to attain the dataset.

Then we format it into two `.tsv` files (`recipe1m_train.tsv` and `recipe1m_val.tsv`) as follows by running the `recipe1m_processing.py`:
```
caption image
This is food title: Kombu Tea Grilled Chicken Thigh This is ingredient: ['2 chicken thighs', '2 tsp kombu tea', '1 white pepper'] This is instruction: ['pierce the skin of the chicken with a fork or knife.', 'sprinkle with kombu tea evenly on both sides of the chicken, about 1 teaspoon per chicken thigh.', 'brown the skin side of the chicken first over high heat until golden brown.', 'sprinkle some pepper on the meat just before flipping over.', 'then brown the other side until golden brown.']      6bdca6e490.jpg
This is food title: Strawberry Rhubarb Dump Cake This is ingredient: ['6 8 cups fresh rhubarb, or', '6 8 cups frozen rhubarb, thawed', '1 12 cups granulated sugar', '6 ounces strawberry jell o gelatin dessert', '1 white cake mix', '1 12 cups water', '12 cup butter or 12 cup margarine, melted'] This is instruction: ['put ingredients in a buttered 9 x 12 x 2 inch pan in even layers in the order that they are given do not mix.', 'bake in a 350 oven for 1 hour.'] 6409eab844.jpg
This is food title: Yogurt Parfaits This is ingredient: ['8 ounces, weight light fat free vanilla yogurt', '1 cup fresh sliced strawberries', '1/4 cups low fat granola'] This is instruction: ['layer all ingredients in a serving dish.']     a1374cdd98.jpg
```
where each line contains the caption followed by the filename of the image files. Save these `.tsv` files into the `dataset/` folder (the default names expected are `recipe1m_train.tsv` and `recipe1m_val.tsv`). The repo contains two placeholder files with a few examples, and you will have to replace them with the appropriate data.

The corresponding image files should be saved in the `data/` directory. The directory can be changed with the `--image-dir` runtime flag.

### Precomputing Text Embeddings

In addition to downloading the images, we need the embeddings from the text encoder of Stable Diffusion to train. We precompute this ahead of time in order to improve training time throughput. To do so, run the following script:

```
python scripts/preprocess_sd_embeddings.py  datasets/recipe1m_val.tsv data/recipe1m/validation
```

This will precompute embeddings from the captions in `recipe1m_val.tsv`, and save the results to `data/recipe1m/validation/clip_embs`.

### Starting a Training Job

After preprocessing the data, we can finally start a training job with the following command line flag:

```
randport=$(shuf -i8000-9999 -n1)  # Generate a random port number
python -u main.py \
    --dist-url "tcp://127.0.0.1:${randport}" --dist-backend 'nccl' \
    --epochs=5  --steps_per_epoch=1400 \
    --multiprocessing-distributed --world-size 1 --rank 0 \
    --dataset=recipe1m  --val-dataset=recipe1m \
    --exp-name='chefFusion_maxlen300_6b_epoch5_steps1400_batchsize16_gpu2(name whatever you want)' --image-dir='data/'  --log-base-dir='runs/' \
    --batch-size=16  --val-batch-size=16 \
    --precision='bf16'  --print-freq=100 --max-len=300 > chefFusion_maxlen300_6b_epoch5_step1400_batchsize16_gpu2(name whatever you want)
```


The default hyperparameters in `main.py` should reproduce our main results in the paper. You can also specify the device of GPU by adding something like `CUDA_VISIBLE_DEVICES=0,1` in front of `python -u main.py`. We train on 2 A100 GPUs for 1 day. For GPUs with smaller memory available, you might need to reduce the batch size, enable gradient accumulation, or adjust hyperparameters to get good performance. You may also have to disable NCCL P2P with export NCCL_P2P_DISABLE=1 if you run into issues.



## Pruning the Checkpoint

As ChefFusion only consists of a few pretrained linear layers and the `[IMG]` embeddings, we can discard most of the pretrained weights to save on disk space. If you have trained a new model, and wish to do so, you can use `chefFusion/prune_model_ckpt.py` file to prune the model weights, and format the ckpt as required by `chefFusion/models.py`:

```
python scripts/prune_model_ckpt.py  runs/chefFusion_maxlen300_6b_epoch5_step1400_batchsize16_gpu2
```

We used the same script to create the weights in the `checkpoints/` directory.


Our code is modified from [GILL](https://github.com/kohjingyu/gill/tree/main).
